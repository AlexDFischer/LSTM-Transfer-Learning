{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load some books from project Gutenberg into strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "# Characters are represented by 1-hot vectors of size 128\n",
    "char_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "import string\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import LSTM\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file Thornton Waldo Burgess___The Adventures of Jimmy Skunk.txt\n",
      "read 86954 characters\n",
      "reading file Louisa May Alcott___Shawl-Straps.txt\n",
      "read 257608 characters\n",
      "reading file Andrew Lang___John Knox and the Reformation.txt\n",
      "read 488156 characters\n",
      "reading file Sir Richard Francis Burton___To the Gold Coast for Gold, Volume 1.txt\n",
      "read 514103 characters\n",
      "reading file Daniel Defoe___The History of the Devil.txt\n",
      "read 721216 characters\n",
      "Counter({' ': 334089, 'e': 189864, 'n': 143002, 't': 138085, 'a': 123952, 'o': 115740, 'i': 101535, 's': 95039, 'h': 93515, 'r': 91812, 'd': 63674, 'l': 61959, '\\\\': 43719, 'u': 40924, 'c': 36985, 'f': 34188, ',': 33755, 'm': 33514, 'w': 30568, 'g': 27188, 'y': 27143, 'p': 25581, 'b': 22758, 'v': 16037, '.': 14271, '_': 11357, 'k': 9653, \"'\": 7853, 'T': 5103, 'S': 4880, '-': 4789, 'I': 4730, 'A': 4378, ';': 4296, 'C': 4223, 'M': 4133, 'D': 3790, '\"': 3653, 'x': 3487, 'P': 3446, 'H': 3002, 'B': 2914, 'F': 2461, 'E': 2446, 'W': 2307, '1': 2301, 'L': 2278, 'R': 2129, 'G': 2087, 'K': 1714, '2': 1569, 'N': 1533, '5': 1501, 'O': 1487, 'q': 1430, 'J': 1346, '0': 1320, 'j': 1281, '(': 1156, ')': 1156, ':': 947, 'z': 938, '3': 900, '6': 892, '4': 854, '8': 832, '}': 822, '{': 811, '7': 728, 'V': 654, '?': 594, '9': 593, 'U': 419, '!': 412, 'Y': 370, 'Q': 361, '[': 215, ']': 215, 'X': 109, '&': 76, '/': 76, 'Z': 70, '*': 34, '|': 14, '$': 6, '=': 5, '+': 4})\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed=12345)\n",
    "\n",
    "# replaces special characters with their close equivalents in order to simplify the characters that appear\n",
    "def clean_text(text):\n",
    "    return str(unicodedata.normalize('NFD', text).encode('ascii', 'ignore'))\n",
    "\n",
    "gutenberg_dir = 'Gutenberg/txt/'\n",
    "gutenberg_files = os.listdir(gutenberg_dir)\n",
    "myfiles = np.random.choice(gutenberg_files, 5)\n",
    "mystrings = []\n",
    "counters = []\n",
    "combined_counter = Counter()\n",
    "for file in myfiles:\n",
    "    print('reading file %s' % file)\n",
    "    myfile = open(gutenberg_dir + file, 'r')\n",
    "    file_text = clean_text(myfile.read())\n",
    "    print('read %d characters' % len(file_text))\n",
    "    mystrings += [file_text]\n",
    "    myfile.close()\n",
    "    counter = Counter(file_text)\n",
    "    counters += [counter]\n",
    "    combined_counter += counter\n",
    "\n",
    "print(combined_counter)\n",
    "\n",
    "for key in combined_counter.keys():\n",
    "    if ord(key) >= 128:\n",
    "        print('invalid character value found: %s has numeric value %d', key, ord(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train an LSTM on this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts a list of N strings of length T into a numpy array of 1-hot vectors\n",
    "# input size: (N, T)\n",
    "# output size: (T, N, 128)\n",
    "i128 = np.eye(128)\n",
    "def char_to_ix(texts):\n",
    "    ords = np.array([[ord(char) for char in text] for text in texts], dtype=int)\n",
    "    return i128[ords].transpose((1, 0, 2))\n",
    "\n",
    "# converts a list of N strings of length T into a numpy array of length (T, N)\n",
    "def char_to_array(texts):\n",
    "    ords = np.array([[ord(char) for char in text] for text in texts], dtype=int)\n",
    "    return ords.transpose((1, 0))\n",
    "\n",
    "#data = char_to_ix(mystrings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_stacks):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(char_dim, hidden_dim, num_layers=num_stacks)\n",
    "        \n",
    "        # The linear layer that maps from hidden state space to character space\n",
    "        self.hidden2char = nn.Linear(hidden_dim, char_dim)\n",
    "        self.init_hidden_zeros(1)\n",
    "    \n",
    "    def init_hidden_zeros(self, minibatch_size):\n",
    "        self.init_hidden(torch.zeros((self.lstm.num_layers, minibatch_size, self.hidden_dim)), torch.zeros((self.lstm.num_layers, minibatch_size, self.hidden_dim)))\n",
    "    \n",
    "    def init_hidden(self, h, c):\n",
    "        self.hidden = (h, c)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text should be of size (T, N, char_dim)\n",
    "        # returns character scores of size (T, N, char_dim)\n",
    "        \n",
    "        hs, self.hidden = self.lstm(text, self.hidden)\n",
    "        char_space = self.hidden2char(hs)\n",
    "        return char_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(model, loss_func, data_ix, data_array):\n",
    "    model.lstm.eval()\n",
    "    model.init_hidden_zeros(data_ix.shape[1])\n",
    "    sequence_in = data_ix[:-1, :, :]\n",
    "    minibatch_size = data_ix.shape[1]\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, char_in in enumerate(sequence_in):\n",
    "            char_scores = model(char_in.view(1, minibatch_size, -1))\n",
    "            loss += loss_func(char_scores.view(-1, char_dim), data_array[i+1,:])\n",
    "    model.lstm.train()\n",
    "    return loss / len(sequence_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1654427\n",
      "206804\n",
      "206806\n"
     ]
    }
   ],
   "source": [
    "# free some memory if possible\n",
    "train_data = None\n",
    "val_data = None\n",
    "test_data = None\n",
    "val_data_ix = None\n",
    "val_data_array = None\n",
    "test_data_ix = None\n",
    "test_data_array = None\n",
    "gc.collect()\n",
    "\n",
    "train_data = ''\n",
    "val_data = ''\n",
    "test_data = ''\n",
    "\n",
    "for string in mystrings:\n",
    "    train_data += string[:len(string) * 8 // 10]\n",
    "    val_data += string[len(string) * 8 // 10:len(string) * 9 // 10]\n",
    "    test_data += string[len(string) * 9 // 10:]\n",
    "\n",
    "train_data_ix = torch.tensor(char_to_ix([train_data]), dtype=torch.float)\n",
    "train_data_array = torch.tensor(char_to_array([train_data])).view(-1, 1)\n",
    "\n",
    "val_data_ix = torch.tensor(char_to_ix([val_data]), dtype=torch.float)\n",
    "val_data_array = torch.tensor(char_to_array([val_data])).view(-1, 1)\n",
    "\n",
    "test_data_ix = torch.tensor(char_to_ix([test_data]), dtype=torch.float)\n",
    "test_data_array = torch.tensor(char_to_array([test_data])).view(-1, 1)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "print(len(test_data))\n",
    "\n",
    "\n",
    "#train_data = mystrings[0][:90000]\n",
    "#\n",
    "## data_ix is of shape (T, N, char_dim) while data_array is of shape (T, N)\n",
    "#train_data_ix = torch.tensor(char_to_ix([train_data]), dtype=torch.float)\n",
    "#train_data_array = torch.tensor(char_to_array([train_data])).view(-1, 1)\n",
    "#\n",
    "#val_data = mystrings[0][-100000:-50000]\n",
    "#\n",
    "## data_ix is of shape (T, N, char_dim) while data_array is of shape (T, N)\n",
    "#val_data_ix = torch.tensor(char_to_ix([val_data]), dtype=torch.float)\n",
    "#val_data_array = torch.tensor(char_to_array([val_data])).view(-1, 1)\n",
    "#\n",
    "#test_data = mystrings[0][-50000:]\n",
    "#\n",
    "## data_ix is of shape (T, N, char_dim) while data_array is of shape (T, N)\n",
    "#test_data_ix = torch.tensor(char_to_ix([test_data]), dtype=torch.float)\n",
    "#test_data_array = torch.tensor(char_to_array([test_data])).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on epoch 0\n",
      "\ton iteration 205 / 206\n",
      "\ttraining loss = 2.170997\n",
      "\tvalidation loss = 2.238158\n",
      "on epoch 1\n",
      "\ton iteration 205 / 206\n",
      "\ttraining loss = 2.015709\n",
      "\tvalidation loss = 2.088683\n",
      "on epoch 2\n",
      "\ton iteration 205 / 206\n",
      "\ttraining loss = 1.896007\n",
      "\tvalidation loss = 1.986391\n",
      "on epoch 3\n",
      "\ton iteration 205 / 206\n",
      "\ttraining loss = 1.838398\n",
      "\tvalidation loss = 1.927146\n",
      "on epoch 4\n",
      "\ton iteration 205 / 206\n",
      "\ttraining loss = 1.805333\n",
      "\tvalidation loss = 1.893997\n",
      "on epoch 5\n",
      "\ton iteration 205 / 206\n",
      "\ttraining loss = 1.773635\n",
      "\tvalidation loss = 1.856635\n",
      "on epoch 6\n",
      "\ton iteration 205 / 206\n",
      "\ttraining loss = 1.752420\n",
      "\tvalidation loss = 1.829991\n",
      "on epoch 7\n",
      "\ton iteration 205 / 206\n",
      "\ttraining loss = 1.737824\n",
      "\tvalidation loss = 1.809633\n",
      "on epoch 8\n",
      "\ton iteration 205 / 206\n",
      "\ttraining loss = 1.723420\n",
      "\tvalidation loss = 1.791808\n",
      "on epoch 9\n",
      "\ton iteration 205 / 206\n",
      "\ttraining loss = 1.716309\n",
      "\tvalidation loss = 1.781031\n",
      "on epoch 10\n",
      "\ton iteration 205 / 206\n",
      "\ttraining loss = 1.707075\n",
      "\tvalidation loss = 1.769508\n",
      "on epoch 11\n",
      "\ton iteration 205 / 206\n",
      "\ttraining loss = 1.695988\n",
      "\tvalidation loss = 1.756738\n",
      "on epoch 12\n",
      "\ton iteration 205 / 206\n",
      "\ttraining loss = 1.683504\n",
      "\tvalidation loss = 1.743921\n",
      "on epoch 13\n",
      "\ton iteration 205 / 206\n",
      "\ttraining loss = 1.673422\n",
      "\tvalidation loss = 1.733155\n",
      "on epoch 14\n",
      "\ton iteration 205 / 206\n",
      "\ttraining loss = 1.665728\n",
      "\tvalidation loss = 1.725434\n",
      "on epoch 15\n",
      "\ton iteration 205 / 206\n",
      "\ttraining loss = 1.656055\n",
      "\tvalidation loss = 1.715864\n",
      "on epoch 16\n",
      "\ton iteration 205 / 206\n",
      "\ttraining loss = 1.645965\n",
      "\tvalidation loss = 1.706959\n",
      "on epoch 17\n",
      "\ton iteration 205 / 206\n",
      "\ttraining loss = 1.641855\n",
      "\tvalidation loss = 1.702309\n",
      "on epoch 18\n",
      "\ton iteration 205 / 206\n",
      "\ttraining loss = 1.626770\n",
      "\tvalidation loss = 1.688216\n",
      "on epoch 19\n",
      "\ton iteration 205 / 206\n",
      "\ttraining loss = 1.629209\n",
      "\tvalidation loss = 1.690043\n",
      "on epoch 20\n",
      "\ton iteration 205 / 206\n",
      "\ttraining loss = 1.623515\n",
      "\tvalidation loss = 1.687196\n",
      "on epoch 21\n",
      "\ton iteration 205 / 206\n",
      "\ttraining loss = 1.618575\n",
      "\tvalidation loss = 1.684389\n",
      "on epoch 22\n",
      "\ton iteration 205 / 206\n",
      "\ttraining loss = 1.614914\n",
      "\tvalidation loss = 1.684167\n",
      "on epoch 23\n",
      "\ton iteration 205 / 206\n",
      "\ttraining loss = 1.609911\n",
      "\tvalidation loss = 1.683201\n",
      "on epoch 24\n",
      "\ton iteration 205 / 206\n",
      "\ttraining loss = 1.606072\n",
      "\tvalidation loss = 1.682063\n",
      "on epoch 25\n",
      "\ton iteration 205 / 206\n",
      "\ttraining loss = 1.609762\n",
      "\tvalidation loss = 1.687622\n",
      "on epoch 26\n",
      "\ton iteration 205 / 206\n",
      "\ttraining loss = 1.600290\n",
      "\tvalidation loss = 1.680066\n",
      "on epoch 27\n",
      "\ton iteration 205 / 206\n",
      "\ttraining loss = 1.597727\n",
      "\tvalidation loss = 1.680467\n",
      "on epoch 28\n",
      "\ton iteration 205 / 206\n",
      "\ttraining loss = 1.598788\n",
      "\tvalidation loss = 1.681647\n",
      "on epoch 29\n",
      "\ton iteration 205 / 206\n",
      "\ttraining loss = 1.603893\n",
      "\tvalidation loss = 1.683277\n",
      "\ttraining loss = 1.721142\n",
      "\tvalidation loss = 1.701210\n"
     ]
    }
   ],
   "source": [
    "model = MyLSTM(64, 1)\n",
    "#model.load_state_dict(torch.load('model_checkpoint_h100_3layer_epoch9'))\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "seq_len = 1000\n",
    "minibatch_size = 8\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(30):\n",
    "    print('on epoch %d' % epoch)\n",
    "    for i in range(len(train_data) // (seq_len * minibatch_size)):\n",
    "        print('\\r\\ton iteration %d / %d' % (i, len(train_data) // (seq_len * minibatch_size)), end='')\n",
    "        model.zero_grad()\n",
    "        model.init_hidden_zeros(minibatch_size)\n",
    "        \n",
    "        sequence_in = torch.zeros((seq_len - 1, minibatch_size, char_dim))\n",
    "        sequence_out = torch.zeros((seq_len - 1, minibatch_size), dtype=torch.long)\n",
    "        for b in range(minibatch_size):\n",
    "            sequence_in[:,b,:] = train_data_ix[seq_len * (i * minibatch_size + b)\n",
    "                                               :\n",
    "                                               seq_len * (i * minibatch_size + b + 1) - 1\n",
    "                                               ,0,:\n",
    "                                              ]\n",
    "            \n",
    "            sequence_out[:,b] =  train_data_array[seq_len * (i * minibatch_size + b) + 1\n",
    "                                                  :\n",
    "                                                  seq_len * (i * minibatch_size + b + 1) \n",
    "                                                  ,0\n",
    "                                                 ]\n",
    "        #sequence_in = train_data_ix[seq_len * i:seq_len * (i + 1) - 1, :, :]\n",
    "        #sequence_out = train_data_array[seq_len * i + 1:seq_len * (i + 1), :]\n",
    "        \n",
    "        char_scores = model(sequence_in)\n",
    "        loss = loss_func(char_scores.view(-1, char_dim), sequence_out.view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "    print()\n",
    "    train_loss = model_loss(model, loss_func, train_data_ix[10000:20000,:,:], train_data_array[10000:20000,:])\n",
    "    val_loss = model_loss(model, loss_func, val_data_ix[:10000,:,:], val_data_array[:10000,:])\n",
    "    print('\\ttraining loss = %f' % train_loss)\n",
    "    print('\\tvalidation loss = %f' % val_loss)\n",
    "    train_losses += [train_loss]\n",
    "    val_losses += [val_loss]\n",
    "    torch.save(model.state_dict(), 'model_checkpoint_2_h64_3layer_epoch' + str(epoch))\n",
    "\n",
    "train_loss = model_loss(model, loss_func, train_data_ix[:100000,:,:], train_data_array[:100000,:])\n",
    "val_loss = model_loss(model, loss_func, val_data_ix, val_data_array)\n",
    "print('\\ttraining loss = %f' % train_loss)\n",
    "print('\\tvalidation loss = %f' % val_loss)\n",
    "train_losses += [train_loss]\n",
    "val_losses += [val_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on epoch 0\n",
      "\ton iteration 205 / 206\n",
      "\ttraining loss = 2.505617\n",
      "\tvalidation loss = 2.541927\n",
      "on epoch 1\n",
      "\ton iteration 25 / 206"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-4da98c0be67d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mchar_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/anaconda3/envs/cs682/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/anaconda3/envs/cs682/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MyLSTM(64, 3)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "seq_len = 1000\n",
    "minibatch_size = 8\n",
    "\n",
    "train_losses_h64_l3 = []\n",
    "val_losses_h64_l3 = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    print('on epoch %d' % epoch)\n",
    "    for i in range(len(train_data) // (seq_len * minibatch_size)):\n",
    "        print('\\r\\ton iteration %d / %d' % (i, len(train_data) // (seq_len * minibatch_size)), end='')\n",
    "        model.zero_grad()\n",
    "        model.init_hidden_zeros(minibatch_size)\n",
    "        \n",
    "        sequence_in = torch.zeros((seq_len - 1, minibatch_size, char_dim))\n",
    "        sequence_out = torch.zeros((seq_len - 1, minibatch_size), dtype=torch.long)\n",
    "        for b in range(minibatch_size):\n",
    "            sequence_in[:,b,:] = train_data_ix[seq_len * (i * minibatch_size + b)\n",
    "                                               :\n",
    "                                               seq_len * (i * minibatch_size + b + 1) - 1\n",
    "                                               ,0,:\n",
    "                                              ]\n",
    "            \n",
    "            sequence_out[:,b] =  train_data_array[seq_len * (i * minibatch_size + b) + 1\n",
    "                                                  :\n",
    "                                                  seq_len * (i * minibatch_size + b + 1) \n",
    "                                                  ,0\n",
    "                                                 ]\n",
    "        #sequence_in = train_data_ix[seq_len * i:seq_len * (i + 1) - 1, :, :]\n",
    "        #sequence_out = train_data_array[seq_len * i + 1:seq_len * (i + 1), :]\n",
    "        \n",
    "        char_scores = model(sequence_in)\n",
    "        loss = loss_func(char_scores.view(-1, char_dim), sequence_out.view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "    print()\n",
    "    train_loss = model_loss(model, loss_func, train_data_ix[10000:20000,:,:], train_data_array[10000:20000,:])\n",
    "    val_loss = model_loss(model, loss_func, val_data_ix[:10000,:,:], val_data_array[:10000,:])\n",
    "    print('\\ttraining loss = %f' % train_loss)\n",
    "    print('\\tvalidation loss = %f' % val_loss)\n",
    "    train_losses_h64_l3 += [train_loss]\n",
    "    val_losses_h64_l3 += [val_loss]\n",
    "    torch.save(model.state_dict(), 'model_checkpoint_2_h64_3layer_epoch' + str(epoch))\n",
    "\n",
    "train_loss = model_loss(model, loss_func, train_data_ix[:100000,:,:], train_data_array[:100000,:])\n",
    "val_loss = model_loss(model, loss_func, val_data_ix, val_data_array)\n",
    "print('training loss = %f' % train_loss)\n",
    "print('validation loss = %f' % val_loss)\n",
    "train_losses_h64_l3 += [train_loss]\n",
    "val_losses_h64_l3 += [val_loss]\n",
    "\n",
    "##############################\n",
    "\n",
    "model = MyLSTM(64, 3)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "seq_len = 1000\n",
    "minibatch_size = 8\n",
    "\n",
    "train_losses_h64_l3 = []\n",
    "val_losses_h64_l3 = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    print('on epoch %d' % epoch)\n",
    "    for i in range(len(train_data) // (seq_len * minibatch_size)):\n",
    "        print('\\r\\ton iteration %d / %d' % (i, len(train_data) // (seq_len * minibatch_size)), end='')\n",
    "        model.zero_grad()\n",
    "        model.init_hidden_zeros(minibatch_size)\n",
    "        \n",
    "        sequence_in = torch.zeros((seq_len - 1, minibatch_size, char_dim))\n",
    "        sequence_out = torch.zeros((seq_len - 1, minibatch_size), dtype=torch.long)\n",
    "        for b in range(minibatch_size):\n",
    "            sequence_in[:,b,:] = train_data_ix[seq_len * (i * minibatch_size + b)\n",
    "                                               :\n",
    "                                               seq_len * (i * minibatch_size + b + 1) - 1\n",
    "                                               ,0,:\n",
    "                                              ]\n",
    "            \n",
    "            sequence_out[:,b] =  train_data_array[seq_len * (i * minibatch_size + b) + 1\n",
    "                                                  :\n",
    "                                                  seq_len * (i * minibatch_size + b + 1) \n",
    "                                                  ,0\n",
    "                                                 ]\n",
    "        #sequence_in = train_data_ix[seq_len * i:seq_len * (i + 1) - 1, :, :]\n",
    "        #sequence_out = train_data_array[seq_len * i + 1:seq_len * (i + 1), :]\n",
    "        \n",
    "        char_scores = model(sequence_in)\n",
    "        loss = loss_func(char_scores.view(-1, char_dim), sequence_out.view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "    print()\n",
    "    train_loss = model_loss(model, loss_func, train_data_ix[10000:20000,:,:], train_data_array[10000:20000,:])\n",
    "    val_loss = model_loss(model, loss_func, val_data_ix[:10000,:,:], val_data_array[:10000,:])\n",
    "    print('\\ttraining loss = %f' % train_loss)\n",
    "    print('\\tvalidation loss = %f' % val_loss)\n",
    "    train_losses_h64_l3 += [train_loss]\n",
    "    val_losses_h64_l3 += [val_loss]\n",
    "    torch.save(model.state_dict(), 'model_checkpoint_2_h64_3layer_epoch' + str(epoch))\n",
    "\n",
    "train_loss = model_loss(model, loss_func, train_data_ix[:100000,:,:], train_data_array[:100000,:])\n",
    "val_loss = model_loss(model, loss_func, val_data_ix, val_data_array)\n",
    "print('training loss = %f' % train_loss)\n",
    "print('validation loss = %f' % val_loss)\n",
    "train_losses_h64_l3 += [train_loss]\n",
    "val_losses_h64_l3 += [val_loss]\n",
    "\n",
    "##############################\n",
    "\n",
    "\n",
    "model = MyLSTM(128, 3)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "seq_len = 1000\n",
    "minibatch_size = 8\n",
    "\n",
    "train_losses_h128_l3 = []\n",
    "val_losses_h128_l3 = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    print('on epoch %d' % epoch)\n",
    "    for i in range(len(train_data) // (seq_len * minibatch_size)):\n",
    "        print('\\r\\ton iteration %d / %d' % (i, len(train_data) // (seq_len * minibatch_size)), end='')\n",
    "        model.zero_grad()\n",
    "        model.init_hidden_zeros(minibatch_size)\n",
    "        \n",
    "        sequence_in = torch.zeros((seq_len - 1, minibatch_size, char_dim))\n",
    "        sequence_out = torch.zeros((seq_len - 1, minibatch_size), dtype=torch.long)\n",
    "        for b in range(minibatch_size):\n",
    "            sequence_in[:,b,:] = train_data_ix[seq_len * (i * minibatch_size + b)\n",
    "                                               :\n",
    "                                               seq_len * (i * minibatch_size + b + 1) - 1\n",
    "                                               ,0,:\n",
    "                                              ]\n",
    "            \n",
    "            sequence_out[:,b] =  train_data_array[seq_len * (i * minibatch_size + b) + 1\n",
    "                                                  :\n",
    "                                                  seq_len * (i * minibatch_size + b + 1) \n",
    "                                                  ,0\n",
    "                                                 ]\n",
    "        #sequence_in = train_data_ix[seq_len * i:seq_len * (i + 1) - 1, :, :]\n",
    "        #sequence_out = train_data_array[seq_len * i + 1:seq_len * (i + 1), :]\n",
    "        \n",
    "        char_scores = model(sequence_in)\n",
    "        loss = loss_func(char_scores.view(-1, char_dim), sequence_out.view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "    print()\n",
    "    train_loss = model_loss(model, loss_func, train_data_ix[10000:20000,:,:], train_data_array[10000:20000,:])\n",
    "    val_loss = model_loss(model, loss_func, val_data_ix[:10000,:,:], val_data_array[:10000,:])\n",
    "    print('\\ttraining loss = %f' % train_loss)\n",
    "    print('\\tvalidation loss = %f' % val_loss)\n",
    "    train_losses_h128_l3 += [train_loss]\n",
    "    val_losses_h128_l3 += [val_loss]\n",
    "    torch.save(model.state_dict(), 'model_checkpoint_2_h128_3layer_epoch' + str(epoch))\n",
    "\n",
    "train_loss = model_loss(model, loss_func, train_data_ix[:100000,:,:], train_data_array[:100000,:])\n",
    "val_loss = model_loss(model, loss_func, val_data_ix, val_data_array)\n",
    "print('training loss = %f' % train_loss)\n",
    "print('validation loss = %f' % val_loss)\n",
    "train_losses_h128_l3 += [train_loss]\n",
    "val_losses_h128_l3 += [val_loss]\n",
    "\n",
    "#################################\n",
    "\n",
    "model = MyLSTM(64, 2)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "seq_len = 1000\n",
    "minibatch_size = 8\n",
    "\n",
    "train_losses_h64_l2 = []\n",
    "val_losses_h64_l2 = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    print('on epoch %d' % epoch)\n",
    "    for i in range(len(train_data) // (seq_len * minibatch_size)):\n",
    "        print('\\r\\ton iteration %d / %d' % (i, len(train_data) // (seq_len * minibatch_size)), end='')\n",
    "        model.zero_grad()\n",
    "        model.init_hidden_zeros(minibatch_size)\n",
    "        \n",
    "        sequence_in = torch.zeros((seq_len - 1, minibatch_size, char_dim))\n",
    "        sequence_out = torch.zeros((seq_len - 1, minibatch_size), dtype=torch.long)\n",
    "        for b in range(minibatch_size):\n",
    "            sequence_in[:,b,:] = train_data_ix[seq_len * (i * minibatch_size + b)\n",
    "                                               :\n",
    "                                               seq_len * (i * minibatch_size + b + 1) - 1\n",
    "                                               ,0,:\n",
    "                                              ]\n",
    "            \n",
    "            sequence_out[:,b] =  train_data_array[seq_len * (i * minibatch_size + b) + 1\n",
    "                                                  :\n",
    "                                                  seq_len * (i * minibatch_size + b + 1) \n",
    "                                                  ,0\n",
    "                                                 ]\n",
    "        #sequence_in = train_data_ix[seq_len * i:seq_len * (i + 1) - 1, :, :]\n",
    "        #sequence_out = train_data_array[seq_len * i + 1:seq_len * (i + 1), :]\n",
    "        \n",
    "        char_scores = model(sequence_in)\n",
    "        loss = loss_func(char_scores.view(-1, char_dim), sequence_out.view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "    print()\n",
    "    train_loss = model_loss(model, loss_func, train_data_ix[10000:20000,:,:], train_data_array[10000:20000,:])\n",
    "    val_loss = model_loss(model, loss_func, val_data_ix[:10000,:,:], val_data_array[:10000,:])\n",
    "    print('\\ttraining loss = %f' % train_loss)\n",
    "    print('\\tvalidation loss = %f' % val_loss)\n",
    "    train_losses_h64_l2 += [train_loss]\n",
    "    val_losses_h64_l2 += [val_loss]\n",
    "    torch.save(model.state_dict(), 'model_checkpoint_2_h64_2layer_epoch' + str(epoch))\n",
    "\n",
    "train_loss = model_loss(model, loss_func, train_data_ix[:100000,:,:], train_data_array[:100000,:])\n",
    "val_loss = model_loss(model, loss_func, val_data_ix, val_data_array)\n",
    "print('training loss = %f' % train_loss)\n",
    "print('validation loss = %f' % val_loss)\n",
    "train_losses_h64_l2 += [train_loss]\n",
    "val_losses_h64_l2 += [val_loss]\n",
    "\n",
    "##############################\n",
    "\n",
    "model = MyLSTM(128, 3)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "seq_len = 1000\n",
    "minibatch_size = 8\n",
    "\n",
    "train_losses_h128_l2 = []\n",
    "val_losses_h128_l2 = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    print('on epoch %d' % epoch)\n",
    "    for i in range(len(train_data) // (seq_len * minibatch_size)):\n",
    "        print('\\r\\ton iteration %d / %d' % (i, len(train_data) // (seq_len * minibatch_size)), end='')\n",
    "        model.zero_grad()\n",
    "        model.init_hidden_zeros(minibatch_size)\n",
    "        \n",
    "        sequence_in = torch.zeros((seq_len - 1, minibatch_size, char_dim))\n",
    "        sequence_out = torch.zeros((seq_len - 1, minibatch_size), dtype=torch.long)\n",
    "        for b in range(minibatch_size):\n",
    "            sequence_in[:,b,:] = train_data_ix[seq_len * (i * minibatch_size + b)\n",
    "                                               :\n",
    "                                               seq_len * (i * minibatch_size + b + 1) - 1\n",
    "                                               ,0,:\n",
    "                                              ]\n",
    "            \n",
    "            sequence_out[:,b] =  train_data_array[seq_len * (i * minibatch_size + b) + 1\n",
    "                                                  :\n",
    "                                                  seq_len * (i * minibatch_size + b + 1) \n",
    "                                                  ,0\n",
    "                                                 ]\n",
    "        #sequence_in = train_data_ix[seq_len * i:seq_len * (i + 1) - 1, :, :]\n",
    "        #sequence_out = train_data_array[seq_len * i + 1:seq_len * (i + 1), :]\n",
    "        \n",
    "        char_scores = model(sequence_in)\n",
    "        loss = loss_func(char_scores.view(-1, char_dim), sequence_out.view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "    print()\n",
    "    train_loss = model_loss(model, loss_func, train_data_ix[10000:20000,:,:], train_data_array[10000:20000,:])\n",
    "    val_loss = model_loss(model, loss_func, val_data_ix[:10000,:,:], val_data_array[:10000,:])\n",
    "    print('\\ttraining loss = %f' % train_loss)\n",
    "    print('\\tvalidation loss = %f' % val_loss)\n",
    "    train_losses_h128_l2 += [train_loss]\n",
    "    val_losses_h128_l2 += [val_loss]\n",
    "    torch.save(model.state_dict(), 'model_checkpoint_2_h128_2layer_epoch' + str(epoch))\n",
    "\n",
    "train_loss = model_loss(model, loss_func, train_data_ix[:100000,:,:], train_data_array[:100000,:])\n",
    "val_loss = model_loss(model, loss_func, val_data_ix, val_data_array)\n",
    "print('training loss = %f' % train_loss)\n",
    "print('validation loss = %f' % val_loss)\n",
    "train_losses_h128_l2 += [train_loss]\n",
    "val_losses_h128_l2 += [val_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3714"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'my_test_model')\n",
    "model2 = MyLSTM(100)\n",
    "model2.load_state_dict(torch.load('my_test_model'))\n",
    "\n",
    "#train_loss = model_loss(model2, loss_func, train_data_ix[:50000,:,:], train_data_array[:50000,:])\n",
    "#val_loss = model_loss(model2, loss_func, val_data_ix, val_data_array)\n",
    "#print('\\ttraining loss = %f' % train_loss)\n",
    "#print('\\tvalidation loss = %f' % val_loss)\n",
    "model2 = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/bin/anaconda3/envs/cs682/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Anee and you will have let him goes so much of Hold to the King was a tall that I have never seen the haughance of the King, the King of Navarre, and not the first time to keep me the dead. We will be gone, and that you have been able to me you to the King of Nid-de-Merle, whom I will be a confused of the lady and force of the King of Nid-de-Merle, so dead. We will be a sort of the King father and her own priests were\\\\nhelp me to the King of Navarre and your good young horse the honour of your sa'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = torch.nn.Softmax()\n",
    "chars = range(128)\n",
    "\n",
    "def sample_char(char_scores, temp):\n",
    "    char_scores = softmax(char_scores / temp)\n",
    "    char = np.random.choice(chars, p=char_scores.detach().numpy())\n",
    "    while not chr(char) in string.printable:\n",
    "        char = np.random.choice(chars, p=char_scores.detach().numpy())\n",
    "    return char\n",
    "\n",
    "def sample(model, first_char, init_hidden, T, temp):\n",
    "    result = first_char\n",
    "    cur_char = ord(first_char)\n",
    "    for t in range(T):\n",
    "        one_hot_char = torch.tensor(i128[cur_char], dtype=torch.float).view(1, 1, -1)\n",
    "        char_scores = model(one_hot_char)\n",
    "        cur_char = sample_char(char_scores.view(-1), temp)\n",
    "        result += chr(cur_char)\n",
    "    return result\n",
    "\n",
    "sample(model, 'A', torch.zeros((1, 1, model.hidden_dim)), 500, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
