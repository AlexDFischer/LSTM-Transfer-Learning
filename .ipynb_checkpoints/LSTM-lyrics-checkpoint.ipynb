{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "# Characters are represented by 1-hot vectors of size 128\n",
    "char_dim = 128\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "import string\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import LSTM\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replaces special characters with their close equivalents in order to simplify the characters that appear\n",
    "def clean_text(text):\n",
    "    return str(unicodedata.normalize('NFD', text).encode('ascii', 'ignore')).replace('\\\\n', '\\n')\n",
    "\n",
    "file = open('beatles.txt', 'r')\n",
    "beatleslyrics = ''\n",
    "# ignore lines that consist of 'TOP'\n",
    "line = file.readline()\n",
    "while line != '':\n",
    "    beatleslyrics += line\n",
    "    line = file.readline()\n",
    "file.close()\n",
    "beatleslyrics = clean_text(beatleslyrics)\n",
    "print('read %d characters' % len(beatleslyrics))\n",
    "char_counter = Counter(beatleslyrics)\n",
    "print(char_counter)\n",
    "\n",
    "for key in char_counter.keys():\n",
    "    if ord(key) >= 128:\n",
    "        print('invalid character value found: %s has numeric value %d', key, ord(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts a list of N strings of length <=T into a numpy array of 1-hot vectors\n",
    "# input: list of length N; max length of any string in the list is T\n",
    "# output size: (T, N, 128)\n",
    "i128 = np.eye(128)\n",
    "def char_to_ix(texts):\n",
    "    T = max([len(text) for text in texts])\n",
    "    ords = np.zeros((T, len(texts)), dtype=int)\n",
    "    for n, text in enumerate(texts):\n",
    "        ords[:len(text), n] = [ord(char) for char in text]\n",
    "    return i128[ords]\n",
    "\n",
    "# converts a list of N strings of length <=T into a numpy array of length (T, N).\n",
    "# Zero-pads shorter strings.\n",
    "def char_to_array(texts):\n",
    "    T = max([len(text) for text in texts])\n",
    "    result = np.zeros((T, len(texts)), dtype=int)\n",
    "    for n, text in enumerate(texts):\n",
    "        result[:len(text), n] = [ord(char) for char in text]\n",
    "    return result\n",
    "    #ords = np.array([[ord(char) for char in text] for text in texts], dtype=int)\n",
    "    #return ords.transpose((1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free some memory if possible\n",
    "train_data = None\n",
    "val_data = None\n",
    "test_data = None\n",
    "val_data_ix = None\n",
    "val_data_array = None\n",
    "test_data_ix = None\n",
    "test_data_array = None\n",
    "gc.collect()\n",
    "\n",
    "train_data = ''\n",
    "val_data = ''\n",
    "test_data = ''\n",
    "\n",
    "# the string TOP separates all songs\n",
    "songs = beatleslyrics.split('TOP\\n')\n",
    "print('number of songs: %d' % len(songs))\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(songs)\n",
    "\n",
    "train_data = songs[:len(songs) * 34 // 100]\n",
    "val_data   = songs[len(songs) * 34 // 100:len(songs) * 67 // 100]\n",
    "test_data  = songs[len(songs) * 67 // 100:]\n",
    "\n",
    "train_data_ix = torch.tensor(char_to_ix(train_data), dtype=torch.float)\n",
    "train_data_array = torch.tensor(char_to_array(train_data))\n",
    "\n",
    "print(train_data_ix.shape)\n",
    "print(train_data_array.shape)\n",
    "\n",
    "val_data_ix = torch.tensor(char_to_ix(val_data), dtype=torch.float)\n",
    "val_data_array = torch.tensor(char_to_array(val_data))\n",
    "\n",
    "test_data_ix = torch.tensor(char_to_ix(test_data), dtype=torch.float)\n",
    "test_data_array = torch.tensor(char_to_array(test_data))\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeatlesLSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_stacks):\n",
    "        super(BeatlesLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(char_dim, hidden_dim, num_layers=num_stacks, dropout=0.0)\n",
    "        \n",
    "        # The linear layer that maps from hidden state space to character space\n",
    "        self.hidden2char = nn.Linear(hidden_dim, char_dim)\n",
    "        self.init_hidden_zeros(1)\n",
    "    \n",
    "    def init_hidden_zeros(self, minibatch_size):\n",
    "        self.init_hidden(torch.zeros((self.lstm.num_layers, minibatch_size, self.hidden_dim)), torch.zeros((self.lstm.num_layers, minibatch_size, self.hidden_dim)))\n",
    "    \n",
    "    def init_hidden(self, h, c):\n",
    "        self.hidden = (h, c)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text should be of size (T, N, char_dim)\n",
    "        # returns character scores of size (T, N, char_dim)\n",
    "        \n",
    "        hs, self.hidden = self.lstm(text, self.hidden)\n",
    "        char_space = self.hidden2char(hs)\n",
    "        return char_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(model, loss_func, data_ix, data_array):\n",
    "    model.lstm.eval()\n",
    "    this_minibatch_size = data_ix.shape[1]\n",
    "    model.init_hidden_zeros(this_minibatch_size)\n",
    "    sequence_in = data_ix[:-1, :, :]\n",
    "    #sequence_out = data_array[1:, :]\n",
    "\n",
    "    #char_scores = model(sequence_in)\n",
    "    #loss = loss_func(char_scores.view(-1, char_dim), sequence_out.view(-1))\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, char_in in enumerate(sequence_in):\n",
    "            char_scores = model(char_in.view(1, this_minibatch_size, -1))\n",
    "            loss += loss_func(char_scores.view(-1, char_dim), data_array[i+1,:])\n",
    "    model.lstm.train()\n",
    "    return loss / len(sequence_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, epochs, train_data_ix, train_data_array, val_data_ix, val_data_array,\n",
    "               checkpoint_name=None, minibatch_size=4, optimizer=None):\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    if optimizer == None:\n",
    "        optimizer = optim.RMSprop(model.parameters())\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('on epoch %d' % epoch)\n",
    "        for i in range((train_data_ix.shape[1] + 1) // minibatch_size):\n",
    "            print('\\r\\ton iteration %d / %d' % (i, (train_data_ix.shape[1] + 1) // minibatch_size), end='')\n",
    "            model.zero_grad()\n",
    "\n",
    "            sequence_in = train_data_ix[:-1, i * minibatch_size : (i + 1) * minibatch_size, :]\n",
    "            sequence_out = train_data_array[1:, i * minibatch_size : (i + 1) * minibatch_size]\n",
    "\n",
    "            # the last minibatch might have a different size if minibatch_size doesn't evenly divide the number of songs\n",
    "            this_minibatch_size = sequence_in.shape[1]\n",
    "            model.init_hidden_zeros(this_minibatch_size)\n",
    "\n",
    "            char_scores = model(sequence_in)\n",
    "            loss = loss_func(char_scores.contiguous().view(-1, char_dim), sequence_out.contiguous().view(-1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(gutenberg_model.parameters(), 5)\n",
    "            optimizer.step()\n",
    "        print()\n",
    "        train_loss = model_loss(model, loss_func, train_data_ix, train_data_array)\n",
    "        val_loss = model_loss(model, loss_func, val_data_ix, val_data_array)\n",
    "        print('\\ttraining loss = %f' % train_loss)\n",
    "        print('\\tvalidation loss = %f' % val_loss)\n",
    "        train_losses += [train_loss]\n",
    "        val_losses += [val_loss]\n",
    "        if checkpoint_name != None:\n",
    "            torch.save(model.state_dict(), checkpoint_name + str(epoch))\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
