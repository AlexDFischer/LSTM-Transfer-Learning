{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load some books from project Gutenberg into strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "# Characters are represented by 1-hot vectors of size 128\n",
    "char_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "import string\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import LSTM\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file Charlotte Mary Yonge___The Chaplet of Pearls.txt\n",
      "read 1024073 characters\n",
      "reading file Lyman Frank Baum___A Kidnapped Santa Claus.txt\n",
      "read 20879 characters\n",
      "reading file Edgar Allan Poe___The Fall of the House of Usher.txt\n",
      "read 42717 characters\n",
      "reading file Ambrose Bierce___Shapes of Clay.txt\n",
      "read 276326 characters\n",
      "reading file Louisa May Alcott___Rose in Bloom.txt\n",
      "read 508189 characters\n",
      "Counter({' ': 319728, 'e': 182439, 't': 120095, 'a': 110999, 'o': 104990, 'n': 95020, 'h': 91507, 'i': 87606, 's': 87355, 'r': 83463, 'd': 65156, 'l': 60010, 'u': 41981, '\\n': 39592, 'm': 32782, 'c': 31379, 'w': 31077, 'f': 30514, 'g': 29189, ',': 28357, 'y': 27915, 'p': 21638, 'b': 20837, 'v': 13233, '.': 12169, \"'\": 11077, '\\\\': 11067, 'k': 10401, 'I': 6870, '-': 5413, '\"': 4789, 'T': 4424, 'A': 4268, 'S': 2881, 'M': 2743, 'H': 2471, 'E': 2468, 'B': 2466, ';': 2367, '!': 2207, 'C': 2022, 'R': 1993, 'P': 1955, 'N': 1942, 'x': 1796, 'W': 1784, 'O': 1635, '?': 1530, 'L': 1468, 'D': 1402, 'q': 1240, 'j': 1099, 'F': 1031, 'Y': 847, '_': 832, 'G': 828, ':': 579, 'z': 562, 'U': 526, 'K': 463, 'V': 329, 'J': 319, 'Q': 306, 'X': 177, '(': 172, ')': 172, '1': 59, '8': 32, '2': 21, '5': 16, '7': 14, '9': 12, 'Z': 11, '4': 11, '6': 9, '3': 9, '0': 9, '*': 8, '[': 8, ']': 8, '&': 5})\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed=0)\n",
    "\n",
    "# replaces special characters with their close equivalents in order to simplify the characters that appear\n",
    "def clean_text(text):\n",
    "    return str(unicodedata.normalize('NFD', text).encode('ascii', 'ignore')).replace('\\\\n', '\\n')\n",
    "\n",
    "gutenberg_dir = 'Gutenberg/txt/'\n",
    "gutenberg_files = os.listdir(gutenberg_dir)\n",
    "myfiles = np.random.choice(gutenberg_files, 5)\n",
    "mystrings = []\n",
    "counters = []\n",
    "combined_counter = Counter()\n",
    "for file in myfiles:\n",
    "    print('reading file %s' % file)\n",
    "    myfile = open(gutenberg_dir + file, 'r')\n",
    "    file_text = clean_text(myfile.read())\n",
    "    print('read %d characters' % len(file_text))\n",
    "    mystrings += [file_text]\n",
    "    myfile.close()\n",
    "    counter = Counter(file_text)\n",
    "    counters += [counter]\n",
    "    combined_counter += counter\n",
    "\n",
    "print(combined_counter)\n",
    "\n",
    "for key in combined_counter.keys():\n",
    "    if ord(key) >= 128:\n",
    "        print('invalid character value found: %s has numeric value %d', key, ord(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train an LSTM on this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts a list of N strings of length T into a numpy array of 1-hot vectors\n",
    "# input size: (N, T)\n",
    "# output size: (T, N, 128)\n",
    "i128 = np.eye(128)\n",
    "def char_to_ix(texts):\n",
    "    ords = np.array([[ord(char) for char in text] for text in texts], dtype=int)\n",
    "    return i128[ords].transpose((1, 0, 2))\n",
    "\n",
    "# converts a list of N strings of length T into a numpy array of length (T, N)\n",
    "def char_to_array(texts):\n",
    "    ords = np.array([[ord(char) for char in text] for text in texts], dtype=int)\n",
    "    return ords.transpose((1, 0))\n",
    "\n",
    "#data = char_to_ix(mystrings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_stacks):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(char_dim, hidden_dim, num_layers=num_stacks)\n",
    "        \n",
    "        # The linear layer that maps from hidden state space to character space\n",
    "        self.hidden2char = nn.Linear(hidden_dim, char_dim)\n",
    "        self.init_hidden_zeros(1)\n",
    "    \n",
    "    def init_hidden_zeros(self, minibatch_size):\n",
    "        self.init_hidden(torch.zeros((self.lstm.num_layers, minibatch_size, self.hidden_dim)), torch.zeros((self.lstm.num_layers, minibatch_size, self.hidden_dim)))\n",
    "    \n",
    "    def init_hidden(self, h, c):\n",
    "        self.hidden = (h, c)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text should be of size (T, N, char_dim)\n",
    "        # returns character scores of size (T, N, char_dim)\n",
    "        \n",
    "        hs, self.hidden = self.lstm(text, self.hidden)\n",
    "        char_space = self.hidden2char(hs)\n",
    "        return char_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(model, loss_func, data_ix, data_array):\n",
    "    model.lstm.eval()\n",
    "    model.init_hidden_zeros(data_ix.shape[1])\n",
    "    sequence_in = data_ix[:-1, :, :]\n",
    "    minibatch_size = data_ix.shape[1]\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, char_in in enumerate(sequence_in):\n",
    "            char_scores = model(char_in.view(1, minibatch_size, -1))\n",
    "            loss += loss_func(char_scores.view(-1, char_dim), data_array[i+1,:])\n",
    "    model.lstm.train()\n",
    "    return loss / len(sequence_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1497745\n",
      "187219\n",
      "187220\n"
     ]
    }
   ],
   "source": [
    "# free some memory if possible\n",
    "train_data = None\n",
    "val_data = None\n",
    "test_data = None\n",
    "val_data_ix = None\n",
    "val_data_array = None\n",
    "test_data_ix = None\n",
    "test_data_array = None\n",
    "gc.collect()\n",
    "\n",
    "train_data = ''\n",
    "val_data = ''\n",
    "test_data = ''\n",
    "\n",
    "for string in mystrings:\n",
    "    train_data += string[:len(string) * 8 // 10]\n",
    "    val_data += string[len(string) * 8 // 10:len(string) * 9 // 10]\n",
    "    test_data += string[len(string) * 9 // 10:]\n",
    "\n",
    "train_data_ix = torch.tensor(char_to_ix([train_data]), dtype=torch.float)\n",
    "train_data_array = torch.tensor(char_to_array([train_data])).view(-1, 1)\n",
    "\n",
    "val_data_ix = torch.tensor(char_to_ix([val_data]), dtype=torch.float)\n",
    "val_data_array = torch.tensor(char_to_array([val_data])).view(-1, 1)\n",
    "\n",
    "test_data_ix = torch.tensor(char_to_ix([test_data]), dtype=torch.float)\n",
    "test_data_array = torch.tensor(char_to_array([test_data])).view(-1, 1)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "print(len(test_data))\n",
    "\n",
    "\n",
    "#train_data = mystrings[0][:90000]\n",
    "#\n",
    "## data_ix is of shape (T, N, char_dim) while data_array is of shape (T, N)\n",
    "#train_data_ix = torch.tensor(char_to_ix([train_data]), dtype=torch.float)\n",
    "#train_data_array = torch.tensor(char_to_array([train_data])).view(-1, 1)\n",
    "#\n",
    "#val_data = mystrings[0][-100000:-50000]\n",
    "#\n",
    "## data_ix is of shape (T, N, char_dim) while data_array is of shape (T, N)\n",
    "#val_data_ix = torch.tensor(char_to_ix([val_data]), dtype=torch.float)\n",
    "#val_data_array = torch.tensor(char_to_array([val_data])).view(-1, 1)\n",
    "#\n",
    "#test_data = mystrings[0][-50000:]\n",
    "#\n",
    "## data_ix is of shape (T, N, char_dim) while data_array is of shape (T, N)\n",
    "#test_data_ix = torch.tensor(char_to_ix([test_data]), dtype=torch.float)\n",
    "#test_data_array = torch.tensor(char_to_array([test_data])).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on epoch 0\n",
      "\ton iteration 186 / 187\n",
      "\ttraining loss = 2.759114\n",
      "\tvalidation loss = 2.738409\n",
      "on epoch 1\n",
      "\ton iteration 186 / 187\n",
      "\ttraining loss = 2.401613\n",
      "\tvalidation loss = 2.377035\n",
      "on epoch 2\n",
      "\ton iteration 186 / 187\n",
      "\ttraining loss = 2.215128\n",
      "\tvalidation loss = 2.183825\n",
      "on epoch 3\n",
      "\ton iteration 186 / 187\n",
      "\ttraining loss = 2.061440\n",
      "\tvalidation loss = 2.012751\n",
      "on epoch 4\n",
      "\ton iteration 186 / 187\n",
      "\ttraining loss = 1.942938\n",
      "\tvalidation loss = 1.905285\n",
      "on epoch 5\n",
      "\ton iteration 186 / 187\n",
      "\ttraining loss = 1.866682\n",
      "\tvalidation loss = 1.825751\n",
      "on epoch 6\n",
      "\ton iteration 186 / 187\n",
      "\ttraining loss = 1.821900\n",
      "\tvalidation loss = 1.778414\n",
      "on epoch 7\n",
      "\ton iteration 186 / 187\n",
      "\ttraining loss = 1.775339\n",
      "\tvalidation loss = 1.740070\n",
      "on epoch 8\n",
      "\ton iteration 186 / 187\n",
      "\ttraining loss = 1.751027\n",
      "\tvalidation loss = 1.711234\n",
      "on epoch 9\n",
      "\ton iteration 186 / 187\n",
      "\ttraining loss = 1.710356\n",
      "\tvalidation loss = 1.671327\n",
      "on epoch 10\n",
      "\ton iteration 186 / 187\n",
      "\ttraining loss = 1.692259\n",
      "\tvalidation loss = 1.656205\n",
      "on epoch 11\n",
      "\ton iteration 186 / 187\n",
      "\ttraining loss = 1.674236\n",
      "\tvalidation loss = 1.639647\n",
      "on epoch 12\n",
      "\ton iteration 186 / 187\n",
      "\ttraining loss = 1.657669\n",
      "\tvalidation loss = 1.623163\n",
      "on epoch 13\n",
      "\ton iteration 126 / 187"
     ]
    }
   ],
   "source": [
    "model = MyLSTM(64, 3)\n",
    "model.lstm.dropout=0.2\n",
    "#model.load_state_dict(torch.load('model_checkpoint_h100_3layer_epoch9'))\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.parameters())\n",
    "seq_len = 1000\n",
    "minibatch_size = 8\n",
    "\n",
    "train_losses_dropout = []\n",
    "val_losses_dropout = []\n",
    "\n",
    "for epoch in range(30):\n",
    "    print('on epoch %d' % epoch)\n",
    "    for i in range(len(train_data) // (seq_len * minibatch_size)):\n",
    "        print('\\r\\ton iteration %d / %d' % (i, len(train_data) // (seq_len * minibatch_size)), end='')\n",
    "        model.zero_grad()\n",
    "        model.init_hidden_zeros(minibatch_size)\n",
    "        \n",
    "        sequence_in = torch.zeros((seq_len - 1, minibatch_size, char_dim))\n",
    "        sequence_out = torch.zeros((seq_len - 1, minibatch_size), dtype=torch.long)\n",
    "        for b in range(minibatch_size):\n",
    "            sequence_in[:,b,:] = train_data_ix[seq_len * (i * minibatch_size + b)\n",
    "                                               :\n",
    "                                               seq_len * (i * minibatch_size + b + 1) - 1\n",
    "                                               ,0,:\n",
    "                                              ]\n",
    "            \n",
    "            sequence_out[:,b] =  train_data_array[seq_len * (i * minibatch_size + b) + 1\n",
    "                                                  :\n",
    "                                                  seq_len * (i * minibatch_size + b + 1) \n",
    "                                                  ,0\n",
    "                                                 ]\n",
    "        #sequence_in = train_data_ix[seq_len * i:seq_len * (i + 1) - 1, :, :]\n",
    "        #sequence_out = train_data_array[seq_len * i + 1:seq_len * (i + 1), :]\n",
    "        \n",
    "        char_scores = model(sequence_in)\n",
    "        loss = loss_func(char_scores.view(-1, char_dim), sequence_out.view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "    print()\n",
    "    train_loss = model_loss(model, loss_func, train_data_ix[10000:20000,:,:], train_data_array[10000:20000,:])\n",
    "    val_loss = model_loss(model, loss_func, val_data_ix[:10000,:,:], val_data_array[:10000,:])\n",
    "    print('\\ttraining loss = %f' % train_loss)\n",
    "    print('\\tvalidation loss = %f' % val_loss)\n",
    "    train_losses_dropout += [train_loss]\n",
    "    val_losses_dropout += [val_loss]\n",
    "    torch.save(model.state_dict(), 'model_checkpoint_2_h64_3layer_dropout_epoch' + str(epoch))\n",
    "\n",
    "train_loss = model_loss(model, loss_func, train_data_ix[:100000,:,:], train_data_array[:100000,:])\n",
    "val_loss = model_loss(model, loss_func, val_data_ix, val_data_array)\n",
    "print('\\ttraining loss = %f' % train_loss)\n",
    "print('\\tvalidation loss = %f' % val_loss)\n",
    "train_losses_dropout += [train_loss]\n",
    "val_losses_dropout += [val_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on epoch 0\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 2.877763\n",
      "\tvalidation loss = 2.956334\n",
      "on epoch 1\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 2.323029\n",
      "\tvalidation loss = 2.362127\n",
      "on epoch 2\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 2.082082\n",
      "\tvalidation loss = 2.160674\n",
      "on epoch 3\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.961999\n",
      "\tvalidation loss = 2.034866\n",
      "on epoch 4\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.888387\n",
      "\tvalidation loss = 1.970176\n",
      "on epoch 5\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.802883\n",
      "\tvalidation loss = 1.874738\n",
      "on epoch 6\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.755734\n",
      "\tvalidation loss = 1.831897\n",
      "on epoch 7\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.709172\n",
      "\tvalidation loss = 1.782555\n",
      "on epoch 8\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.666104\n",
      "\tvalidation loss = 1.739648\n",
      "on epoch 9\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.644136\n",
      "\tvalidation loss = 1.724389\n",
      "on epoch 10\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.611974\n",
      "\tvalidation loss = 1.688711\n",
      "on epoch 11\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.609201\n",
      "\tvalidation loss = 1.699067\n",
      "on epoch 12\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.576378\n",
      "\tvalidation loss = 1.658110\n",
      "on epoch 13\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.587341\n",
      "\tvalidation loss = 1.687243\n",
      "on epoch 14\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.563353\n",
      "\tvalidation loss = 1.660233\n",
      "on epoch 15\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.531702\n",
      "\tvalidation loss = 1.617394\n",
      "on epoch 16\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.532832\n",
      "\tvalidation loss = 1.618582\n",
      "on epoch 17\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.509847\n",
      "\tvalidation loss = 1.597694\n",
      "on epoch 18\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.506855\n",
      "\tvalidation loss = 1.594206\n",
      "on epoch 19\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.494019\n",
      "\tvalidation loss = 1.582009\n",
      "on epoch 20\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.503949\n",
      "\tvalidation loss = 1.591409\n",
      "on epoch 21\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.487341\n",
      "\tvalidation loss = 1.576643\n",
      "on epoch 22\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.476234\n",
      "\tvalidation loss = 1.568067\n",
      "on epoch 23\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.485699\n",
      "\tvalidation loss = 1.583185\n",
      "on epoch 24\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.464640\n",
      "\tvalidation loss = 1.557671\n",
      "on epoch 25\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.462005\n",
      "\tvalidation loss = 1.557610\n",
      "on epoch 26\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.468577\n",
      "\tvalidation loss = 1.565373\n",
      "on epoch 27\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.470830\n",
      "\tvalidation loss = 1.567587\n",
      "on epoch 28\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.445928\n",
      "\tvalidation loss = 1.545279\n",
      "on epoch 29\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.449901\n",
      "\tvalidation loss = 1.548314\n",
      "on epoch 30\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.458059\n",
      "\tvalidation loss = 1.561478\n",
      "on epoch 31\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.436589\n",
      "\tvalidation loss = 1.541258\n",
      "on epoch 32\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.459409\n",
      "\tvalidation loss = 1.563863\n",
      "on epoch 33\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.433265\n",
      "\tvalidation loss = 1.541771\n",
      "on epoch 34\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.428510\n",
      "\tvalidation loss = 1.538758\n",
      "on epoch 35\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.427708\n",
      "\tvalidation loss = 1.541724\n",
      "on epoch 36\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.434858\n",
      "\tvalidation loss = 1.547315\n",
      "on epoch 37\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.458709\n",
      "\tvalidation loss = 1.569207\n",
      "on epoch 38\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.416476\n",
      "\tvalidation loss = 1.532524\n",
      "on epoch 39\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.441453\n",
      "\tvalidation loss = 1.553276\n",
      "on epoch 40\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.422124\n",
      "\tvalidation loss = 1.541791\n",
      "on epoch 41\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.421768\n",
      "\tvalidation loss = 1.535122\n",
      "on epoch 42\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.432708\n",
      "\tvalidation loss = 1.550304\n",
      "on epoch 43\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.410574\n",
      "\tvalidation loss = 1.532315\n",
      "on epoch 44\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.427367\n",
      "\tvalidation loss = 1.543719\n",
      "on epoch 45\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.415215\n",
      "\tvalidation loss = 1.538263\n",
      "on epoch 46\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.423357\n",
      "\tvalidation loss = 1.544158\n",
      "on epoch 47\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.424071\n",
      "\tvalidation loss = 1.541497\n",
      "on epoch 48\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.429970\n",
      "\tvalidation loss = 1.554332\n",
      "on epoch 49\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.413564\n",
      "\tvalidation loss = 1.538299\n",
      "on epoch 50\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.411911\n",
      "\tvalidation loss = 1.527652\n",
      "on epoch 51\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.424635\n",
      "\tvalidation loss = 1.547064\n",
      "on epoch 52\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.398232\n",
      "\tvalidation loss = 1.524814\n",
      "on epoch 53\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.428730\n",
      "\tvalidation loss = 1.552476\n",
      "on epoch 54\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.407414\n",
      "\tvalidation loss = 1.532979\n",
      "on epoch 55\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.400144\n",
      "\tvalidation loss = 1.529341\n",
      "on epoch 56\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.425542\n",
      "\tvalidation loss = 1.547517\n",
      "on epoch 57\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.404017\n",
      "\tvalidation loss = 1.526497\n",
      "on epoch 58\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.414236\n",
      "\tvalidation loss = 1.542713\n",
      "on epoch 59\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.393190\n",
      "\tvalidation loss = 1.523236\n",
      "on epoch 60\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.406845\n",
      "\tvalidation loss = 1.538211\n",
      "on epoch 61\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.407025\n",
      "\tvalidation loss = 1.529424\n",
      "on epoch 62\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.412555\n",
      "\tvalidation loss = 1.542008\n",
      "on epoch 63\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.388325\n",
      "\tvalidation loss = 1.517628\n",
      "on epoch 64\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.400110\n",
      "\tvalidation loss = 1.527620\n",
      "on epoch 65\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.412909\n",
      "\tvalidation loss = 1.542727\n",
      "on epoch 66\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.401958\n",
      "\tvalidation loss = 1.530591\n",
      "on epoch 67\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.397802\n",
      "\tvalidation loss = 1.526200\n",
      "on epoch 68\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.392758\n",
      "\tvalidation loss = 1.532594\n",
      "on epoch 69\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.386135\n",
      "\tvalidation loss = 1.524420\n",
      "on epoch 70\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.386768\n",
      "\tvalidation loss = 1.519315\n",
      "on epoch 71\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.400418\n",
      "\tvalidation loss = 1.534797\n",
      "on epoch 72\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.383284\n",
      "\tvalidation loss = 1.524313\n",
      "on epoch 73\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.393350\n",
      "\tvalidation loss = 1.534047\n",
      "on epoch 74\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.400151\n",
      "\tvalidation loss = 1.538264\n",
      "on epoch 75\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.396615\n",
      "\tvalidation loss = 1.541927\n",
      "on epoch 76\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.387402\n",
      "\tvalidation loss = 1.523682\n",
      "on epoch 77\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.390791\n",
      "\tvalidation loss = 1.528057\n",
      "on epoch 78\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.399264\n",
      "\tvalidation loss = 1.529879\n",
      "on epoch 79\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.403820\n",
      "\tvalidation loss = 1.532770\n",
      "on epoch 80\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.385976\n",
      "\tvalidation loss = 1.525515\n",
      "on epoch 81\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.388077\n",
      "\tvalidation loss = 1.527468\n",
      "on epoch 82\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.389645\n",
      "\tvalidation loss = 1.530567\n",
      "on epoch 83\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.389156\n",
      "\tvalidation loss = 1.528980\n",
      "on epoch 84\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.376513\n",
      "\tvalidation loss = 1.521255\n",
      "on epoch 85\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.387156\n",
      "\tvalidation loss = 1.525876\n",
      "on epoch 86\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.387577\n",
      "\tvalidation loss = 1.528187\n",
      "on epoch 87\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.372130\n",
      "\tvalidation loss = 1.517515\n",
      "on epoch 88\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.399744\n",
      "\tvalidation loss = 1.537951\n",
      "on epoch 89\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.375294\n",
      "\tvalidation loss = 1.516412\n",
      "on epoch 90\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.383561\n",
      "\tvalidation loss = 1.524717\n",
      "on epoch 91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.395613\n",
      "\tvalidation loss = 1.529234\n",
      "on epoch 92\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.373100\n",
      "\tvalidation loss = 1.510688\n",
      "on epoch 93\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.376405\n",
      "\tvalidation loss = 1.518935\n",
      "on epoch 94\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.385087\n",
      "\tvalidation loss = 1.526511\n",
      "on epoch 95\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.385967\n",
      "\tvalidation loss = 1.528432\n",
      "on epoch 96\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.374249\n",
      "\tvalidation loss = 1.511094\n",
      "on epoch 97\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.385301\n",
      "\tvalidation loss = 1.524568\n",
      "on epoch 98\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.382267\n",
      "\tvalidation loss = 1.518091\n",
      "on epoch 99\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = 1.395571\n",
      "\tvalidation loss = 1.532990\n",
      "training loss = 1.519025\n",
      "validation loss = 1.607826\n",
      "on epoch 0\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = nan\n",
      "\tvalidation loss = nan\n",
      "on epoch 1\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = nan\n",
      "\tvalidation loss = nan\n",
      "on epoch 2\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = nan\n",
      "\tvalidation loss = nan\n",
      "on epoch 3\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = nan\n",
      "\tvalidation loss = nan\n",
      "on epoch 4\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = nan\n",
      "\tvalidation loss = nan\n",
      "on epoch 5\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = nan\n",
      "\tvalidation loss = nan\n",
      "on epoch 6\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = nan\n",
      "\tvalidation loss = nan\n",
      "on epoch 7\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = nan\n",
      "\tvalidation loss = nan\n",
      "on epoch 8\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = nan\n",
      "\tvalidation loss = nan\n",
      "on epoch 9\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = nan\n",
      "\tvalidation loss = nan\n",
      "on epoch 10\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = nan\n",
      "\tvalidation loss = nan\n",
      "on epoch 11\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = nan\n",
      "\tvalidation loss = nan\n",
      "on epoch 12\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = nan\n",
      "\tvalidation loss = nan\n",
      "on epoch 13\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = nan\n",
      "\tvalidation loss = nan\n",
      "on epoch 14\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = nan\n",
      "\tvalidation loss = nan\n",
      "on epoch 15\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = nan\n",
      "\tvalidation loss = nan\n",
      "on epoch 16\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = nan\n",
      "\tvalidation loss = nan\n",
      "on epoch 17\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = nan\n",
      "\tvalidation loss = nan\n",
      "on epoch 18\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = nan\n",
      "\tvalidation loss = nan\n",
      "on epoch 19\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = nan\n",
      "\tvalidation loss = nan\n",
      "on epoch 20\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = nan\n",
      "\tvalidation loss = nan\n",
      "on epoch 21\n",
      "\ton iteration 102 / 103\n",
      "\ttraining loss = nan\n",
      "\tvalidation loss = nan\n",
      "on epoch 22\n",
      "\ton iteration 14 / 103"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-8108c65d6720>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mchar_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/anaconda3/envs/cs682/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/anaconda3/envs/cs682/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MyLSTM(64, 3)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "seq_len = 1000\n",
    "minibatch_size = 16\n",
    "\n",
    "train_losses_h64_l3 = []\n",
    "val_losses_h64_l3 = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    print('on epoch %d' % epoch)\n",
    "    for i in range(len(train_data) // (seq_len * minibatch_size)):\n",
    "        print('\\r\\ton iteration %d / %d' % (i, len(train_data) // (seq_len * minibatch_size)), end='')\n",
    "        model.zero_grad()\n",
    "        model.init_hidden_zeros(minibatch_size)\n",
    "        \n",
    "        sequence_in = torch.zeros((seq_len - 1, minibatch_size, char_dim))\n",
    "        sequence_out = torch.zeros((seq_len - 1, minibatch_size), dtype=torch.long)\n",
    "        for b in range(minibatch_size):\n",
    "            sequence_in[:,b,:] = train_data_ix[seq_len * (i * minibatch_size + b)\n",
    "                                               :\n",
    "                                               seq_len * (i * minibatch_size + b + 1) - 1\n",
    "                                               ,0,:\n",
    "                                              ]\n",
    "            \n",
    "            sequence_out[:,b] =  train_data_array[seq_len * (i * minibatch_size + b) + 1\n",
    "                                                  :\n",
    "                                                  seq_len * (i * minibatch_size + b + 1) \n",
    "                                                  ,0\n",
    "                                                 ]\n",
    "        #sequence_in = train_data_ix[seq_len * i:seq_len * (i + 1) - 1, :, :]\n",
    "        #sequence_out = train_data_array[seq_len * i + 1:seq_len * (i + 1), :]\n",
    "        \n",
    "        char_scores = model(sequence_in)\n",
    "        loss = loss_func(char_scores.view(-1, char_dim), sequence_out.view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "    print()\n",
    "    train_loss = model_loss(model, loss_func, train_data_ix[10000:20000,:,:], train_data_array[10000:20000,:])\n",
    "    val_loss = model_loss(model, loss_func, val_data_ix[:10000,:,:], val_data_array[:10000,:])\n",
    "    print('\\ttraining loss = %f' % train_loss)\n",
    "    print('\\tvalidation loss = %f' % val_loss)\n",
    "    train_losses_h64_l3 += [train_loss]\n",
    "    val_losses_h64_l3 += [val_loss]\n",
    "    torch.save(model.state_dict(), 'model_checkpoint_2_h64_3layer_epoch' + str(epoch))\n",
    "\n",
    "train_loss = model_loss(model, loss_func, train_data_ix[:100000,:,:], train_data_array[:100000,:])\n",
    "val_loss = model_loss(model, loss_func, val_data_ix, val_data_array)\n",
    "print('training loss = %f' % train_loss)\n",
    "print('validation loss = %f' % val_loss)\n",
    "train_losses_h64_l3 += [train_loss]\n",
    "val_losses_h64_l3 += [val_loss]\n",
    "\n",
    "#################################\n",
    "\n",
    "model = MyLSTM(64, 2)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "seq_len = 1000\n",
    "minibatch_size = 16\n",
    "\n",
    "train_losses_h64_l2 = []\n",
    "val_losses_h64_l2 = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    print('on epoch %d' % epoch)\n",
    "    for i in range(len(train_data) // (seq_len * minibatch_size)):\n",
    "        print('\\r\\ton iteration %d / %d' % (i, len(train_data) // (seq_len * minibatch_size)), end='')\n",
    "        model.zero_grad()\n",
    "        model.init_hidden_zeros(minibatch_size)\n",
    "        \n",
    "        sequence_in = torch.zeros((seq_len - 1, minibatch_size, char_dim))\n",
    "        sequence_out = torch.zeros((seq_len - 1, minibatch_size), dtype=torch.long)\n",
    "        for b in range(minibatch_size):\n",
    "            sequence_in[:,b,:] = train_data_ix[seq_len * (i * minibatch_size + b)\n",
    "                                               :\n",
    "                                               seq_len * (i * minibatch_size + b + 1) - 1\n",
    "                                               ,0,:\n",
    "                                              ]\n",
    "            \n",
    "            sequence_out[:,b] =  train_data_array[seq_len * (i * minibatch_size + b) + 1\n",
    "                                                  :\n",
    "                                                  seq_len * (i * minibatch_size + b + 1) \n",
    "                                                  ,0\n",
    "                                                 ]\n",
    "        #sequence_in = train_data_ix[seq_len * i:seq_len * (i + 1) - 1, :, :]\n",
    "        #sequence_out = train_data_array[seq_len * i + 1:seq_len * (i + 1), :]\n",
    "        \n",
    "        char_scores = model(sequence_in)\n",
    "        loss = loss_func(char_scores.view(-1, char_dim), sequence_out.view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "    print()\n",
    "    train_loss = model_loss(model, loss_func, train_data_ix[10000:20000,:,:], train_data_array[10000:20000,:])\n",
    "    val_loss = model_loss(model, loss_func, val_data_ix[:10000,:,:], val_data_array[:10000,:])\n",
    "    print('\\ttraining loss = %f' % train_loss)\n",
    "    print('\\tvalidation loss = %f' % val_loss)\n",
    "    train_losses_h64_l2 += [train_loss]\n",
    "    val_losses_h64_l2 += [val_loss]\n",
    "    torch.save(model.state_dict(), 'model_checkpoint_2_h64_2layer_epoch' + str(epoch))\n",
    "\n",
    "train_loss = model_loss(model, loss_func, train_data_ix[:100000,:,:], train_data_array[:100000,:])\n",
    "val_loss = model_loss(model, loss_func, val_data_ix, val_data_array)\n",
    "print('training loss = %f' % train_loss)\n",
    "print('validation loss = %f' % val_loss)\n",
    "train_losses_h64_l2 += [train_loss]\n",
    "val_losses_h64_l2 += [val_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum validation loss wsa 1.510688 at epoch 92\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(100)\n",
    "plt.plot(x, train_losses_h64_l3[:-1], label='training loss')\n",
    "plt.plot(x, val_losses_h64_l3[:-1], label='validation loss')\n",
    "print('minimum validation loss wsa %f at epoch %d' % (np.min(val_losses_h64_l3), np.argmin(val_losses_h64_l3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = MyLSTM(64, 3)\n",
    "best_model.load_state_dict(torch.load('model_checkpoint_2_h64_3layer_epoch50'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/bin/anaconda3/envs/cs682/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEr or a Portugueses and selding in the Proper Word, and the _Devil_ to act the Disguise, and he think his Spirit is the _Devil_ has been explain in all the Appearance and proverb to enquire up as him to make a Committen of _Devils_, who would be known a Command, the _Devil_\n",
      "of the Performing to the World.\n",
      "\n",
      "The Story of the Discomet as a great Devil was to our old _Devil_, that they for the World but what he had not to be according to it as to come out of the fair _Devil_ in the Side, and the \n"
     ]
    }
   ],
   "source": [
    "softmax = torch.nn.Softmax()\n",
    "chars = range(128)\n",
    "\n",
    "def sample_char(char_scores, temp):\n",
    "    char_scores = softmax(char_scores / temp)\n",
    "    char = np.random.choice(chars, p=char_scores.detach().numpy())\n",
    "    while not chr(char) in string.printable:\n",
    "        char = np.random.choice(chars, p=char_scores.detach().numpy())\n",
    "    return char\n",
    "\n",
    "def sample(model, first_char, init_hidden, T, temp):\n",
    "    result = first_char\n",
    "    cur_char = ord(first_char)\n",
    "    for t in range(T):\n",
    "        one_hot_char = torch.tensor(i128[cur_char], dtype=torch.float).view(1, 1, -1)\n",
    "        char_scores = model(one_hot_char)\n",
    "        cur_char = sample_char(char_scores.view(-1), temp)\n",
    "        result += chr(cur_char)\n",
    "    return result.replace('\\\\n', '\\n') # I messed up the encoding of newliens\n",
    "\n",
    "print(sample(best_model, 'T', torch.zeros((1, 1, model.hidden_dim)), 500, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
